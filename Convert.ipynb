{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/cadt-02/Object-Detection/ros2_ws/model_- 31 january 2024 11_41.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/__init__.py:1532\u001b[0m\n\u001b[1;32m   1530\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jit \u001b[38;5;28;01mas\u001b[39;00m jit\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg \u001b[38;5;28;01mas\u001b[39;00m linalg\n\u001b[0;32m-> 1532\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hub \u001b[38;5;28;01mas\u001b[39;00m hub\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random \u001b[38;5;28;01mas\u001b[39;00m random\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributions \u001b[38;5;28;01mas\u001b[39;00m distributions\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/hub.py:61\u001b[0m\n\u001b[1;32m     58\u001b[0m         sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm  \u001b[38;5;66;03m# If tqdm is installed use it, otherwise use the fake wrapper\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     tqdm \u001b[38;5;241m=\u001b[39m _Faketqdm\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_monitor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TMonitor, TqdmSynchronisationWarning\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tqdm_pandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm_pandas\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcli\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m main  \u001b[38;5;66;03m# TODO: remove in v5.0.0\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgui\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;28;01mas\u001b[39;00m tqdm_gui  \u001b[38;5;66;03m# TODO: remove in v5.0.0\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:844\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:939\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1037\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the model\n",
    "checkpoint = torch.load('/home/cadt-02/Object-Detection/ros2_ws/model_- 31 january 2024 11_41.pt')\n",
    "model = checkpoint['model']\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cadt-02/.local/lib/python3.8/site-packages/ultralytics/nn/modules/head.py:53: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.dynamic or self.shape != shape:\n",
      "/home/cadt-02/.local/lib/python3.8/site-packages/ultralytics/utils/tal.py:299: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  for i, stride in enumerate(strides):\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    # Convert the model's parameters to float data type\n",
    "    model.float()\n",
    "    \n",
    "    input_tensor = torch.randn(1, 3, 640, 640).to(next(model.parameters()).device).type(model.parameters().__next__().dtype)\n",
    "    torch.onnx.export(model, input_tensor, \"model.onnx\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/06/2024-11:14:57] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "def get_engine(onnx_file_path, engine_file_path):\n",
    "    # Check if the engine exists\n",
    "    if os.path.exists(engine_file_path):\n",
    "        # If it does, load the engine and return\n",
    "        with open(engine_file_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "            return runtime.deserialize_cuda_engine(f.read())\n",
    "    else:\n",
    "        # If it doesn't, we need to build the engine from the ONNX model.\n",
    "        with trt.Builder(TRT_LOGGER) as builder, builder.create_network(1) as network, trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "            config = builder.create_builder_config()\n",
    "            config.max_workspace_size = 1 << 28  # 256MiB\n",
    "            builder.max_batch_size = 1\n",
    "            # Parse the ONNX model\n",
    "            with open(onnx_file_path, 'rb') as model:\n",
    "                if not parser.parse(model.read()):\n",
    "                    print('ERROR: Failed to parse the ONNX file.')\n",
    "                    for error in range(parser.num_errors):\n",
    "                        print(parser.get_error(error))\n",
    "                    return None\n",
    "            # Build and return an engine.\n",
    "            engine = builder.build_engine(network, config)\n",
    "            with open(engine_file_path, \"wb\") as f:\n",
    "                f.write(engine.serialize())\n",
    "            return engine\n",
    "\n",
    "# Specify the paths to the ONNX model and the engine\n",
    "onnx_file_path = 'model.onnx'\n",
    "engine_file_path = 'model.trt'\n",
    "\n",
    "# Get the engine\n",
    "engine = get_engine(onnx_file_path, engine_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/07/2024-16:04:31] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87647/4022393972.py:19: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
      "/tmp/ipykernel_87647/4022393972.py:19: DeprecationWarning: Use network created with NetworkDefinitionCreationFlag::EXPLICIT_BATCH flag instead.\n",
      "  size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
      "/tmp/ipykernel_87647/4022393972.py:20: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
      "/tmp/ipykernel_87647/4022393972.py:27: DeprecationWarning: Use get_tensor_mode instead.\n",
      "  if engine.binding_is_input(binding):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/07/2024-16:04:33] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "[03/07/2024-16:04:33] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "[03/07/2024-16:04:33] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "[03/07/2024-16:04:33] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n",
      "[03/07/2024-16:04:33] [TRT] [W] The getMaxBatchSize() function should not be used with an engine built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. This function will always return 1.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 58800 into shape (1228800,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Assume input_data is your input data\u001b[39;00m\n\u001b[1;32m     33\u001b[0m input_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom_sample(size)\n\u001b[0;32m---> 34\u001b[0m input_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m np\u001b[38;5;241m.\u001b[39mcopyto(inputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], input_data\u001b[38;5;241m.\u001b[39mravel())\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Transfer input data to the GPU.\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:299\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(a, newshape, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m           [5, 6]])\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:58\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 58800 into shape (1228800,)"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "\n",
    "# Load your TensorRT model\n",
    "with open(\"model.trt\", \"rb\") as f, trt.Runtime(trt.Logger()) as runtime:\n",
    "    engine = runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "# Create an execution context\n",
    "context = engine.create_execution_context()\n",
    "\n",
    "# Allocate buffers\n",
    "inputs = []\n",
    "outputs = []\n",
    "bindings = []\n",
    "stream = cuda.Stream()\n",
    "for binding in engine:\n",
    "    size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
    "    dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "    # Allocate host and device buffers\n",
    "    host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "    device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "    # Append the device buffer to device bindings.\n",
    "    bindings.append(int(device_mem))\n",
    "    # Append to the appropriate list.\n",
    "    if engine.binding_is_input(binding):\n",
    "        inputs.append((host_mem, device_mem))\n",
    "    else:\n",
    "        outputs.append((host_mem, device_mem))\n",
    "\n",
    "# Assume input_data is your input data\n",
    "input_data = np.random.random_sample(size)\n",
    "input_data = np.reshape(input_data, inputs[0][0].shape)\n",
    "np.copyto(inputs[0][0], input_data.ravel())\n",
    "\n",
    "# Transfer input data to the GPU.\n",
    "cuda.memcpy_htod_async(inputs[0][1], inputs[0][0], stream)\n",
    "\n",
    "# Run inference\n",
    "context.execute_async(bindings=bindings, stream_handle=stream.handle)\n",
    "\n",
    "# Transfer predictions back from the GPU.\n",
    "cuda.memcpy_dtoh_async(outputs[0][0], outputs[0][1], stream)\n",
    "\n",
    "# Synchronize the stream\n",
    "stream.synchronize()\n",
    "\n",
    "# Our model outputs are now in the outputs[0][0] buffer.\n",
    "predictions = outputs[0][0]\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
